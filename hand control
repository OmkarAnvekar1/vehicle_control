import cv2
import mediapipe as mp
import math
import socket

robotAddressPort = ("192.168.43.161", 12345)

# Initialize MediaPipe Hands
mp_hands = mp.solutions.hands
hands = mp_hands.Hands()

# Initialize VideoCapture
cap = cv2.VideoCapture(0)  # Change the parameter if you have multiple cameras
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)  # Set width to 640
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)  # Set height to 480

# Set the desired window size
cv2.namedWindow('Hand Tracking', cv2.WINDOW_NORMAL)  # Create a resizable window
cv2.resizeWindow('Hand Tracking', 640, 480)  # Set the window size to 640x480

# Get the height and width of the frame
height = 480
width = 640

# Calculate the coordinates of the center of the screen
center_x = width // 2
center_y = height // 2

# Calculate the coordinates of the top left and bottom right corners of the top box
top_box_top_left = (center_x - 50, 0)
top_box_bottom_right = (center_x + 50, 100)

# Calculate the coordinates of the top left and bottom right corners of the bottom box
bottom_box_top_left = (center_x - 50, height - 100)
bottom_box_bottom_right = (center_x + 50, height)

# Calculate the coordinates of the top left and bottom right corners of the left box
left_box_top_left = (0, center_y - 50)
left_box_bottom_right = (100, center_y + 50)

# Calculate the coordinates of the top left and bottom right corners of the right box
right_box_top_left = (width - 100, center_y - 50)
right_box_bottom_right = (width, center_y + 50)

def sendTorobot(move):
    msg4robot = ','.join([move, '60,0,0,0'])
    print(msg4robot)
    bytesToSend = str.encode(msg4robot)
    bufferSize = 1024
    UDPClientSocket = socket.socket(family=socket.AF_INET, type=socket.SOCK_DGRAM)
    UDPClientSocket.sendto(bytesToSend, robotAddressPort)

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Flip the frame horizontally (mirror image)
    frame = cv2.flip(frame, 1)

    # Convert the image to RGB
    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # Process the image with MediaPipe Hands
    results = hands.process(image)

    # If hands are detected, annotate the image and label them
    if results.multi_hand_landmarks:
        for hand_landmarks in results.multi_hand_landmarks:
            # Draw hand landmarks on the frame
            mp_drawing = mp.solutions.drawing_utils
            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

            # Get the hand landmarks
            landmarks = []
            for landmark in hand_landmarks.landmark:
                x, y, _ = image.shape
                landmarks.append((int(landmark.x * y), int(landmark.y * x)))

            # Check if the landmarks are inside the boxes
            if (top_box_top_left[0] < landmarks[9][0] < top_box_bottom_right[0] and
                top_box_top_left[1] < landmarks[9][1] < top_box_bottom_right[1]) and \
               (top_box_top_left[0] < landmarks[13][0] < top_box_bottom_right[0] and
                top_box_top_left[1] < landmarks[13][1] < top_box_bottom_right[1]):
                cv2.putText(frame, f"Forward", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)
                sendTorobot("f")
                print("Forward")
            elif (bottom_box_top_left[0] < landmarks[9][0] < bottom_box_bottom_right[0] and
                  bottom_box_top_left[1] < landmarks[9][1] < bottom_box_bottom_right[1]) and \
                 (bottom_box_top_left[0] < landmarks[13][0] < bottom_box_bottom_right[0] and
                  bottom_box_top_left[1] < landmarks[13][1] < bottom_box_bottom_right[1]):
                cv2.putText(frame, f"Back", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)
                sendTorobot("b")
                print("Back")
            elif (left_box_top_left[0] < landmarks[9][0] < left_box_bottom_right[0] and
                  left_box_top_left[1] < landmarks[9][1] < left_box_bottom_right[1]) and \
                 (left_box_top_left[0] < landmarks[13][0] < left_box_bottom_right[0] and
                  left_box_top_left[1] < landmarks[13][1] < left_box_bottom_right[1]):
                cv2.putText(frame, f"Left", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)
                sendTorobot("l")
                print("Left")
            elif (right_box_top_left[0] < landmarks[9][0] < right_box_bottom_right[0] and
                  right_box_top_left[1] < landmarks[9][1] < right_box_bottom_right[1]) and \
                 (right_box_top_left[0] < landmarks[13][0] < right_box_bottom_right[0] and
                  right_box_top_left[1] < landmarks[13][1] < right_box_bottom_right[1]):
                cv2.putText(frame, f"Right", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)
                sendTorobot("r")
                print("Right")
            else:
                cv2.putText(frame, f"Stop", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)
                sendTorobot("s")
                print("Stop")

    # Draw the boxes on the frame with red color and 2 pixels thickness
    cv2.rectangle(frame, top_box_top_left, top_box_bottom_right, (0, 0, 255), 2)
    cv2.rectangle(frame, bottom_box_top_left, bottom_box_bottom_right, (0, 0, 255), 2)
    cv2.rectangle(frame, left_box_top_left, left_box_bottom_right, (0, 0, 255), 2)
    cv2.rectangle(frame, right_box_top_left, right_box_bottom_right, (0, 0, 255), 2)

    # Display the output
    cv2.imshow('Hand Tracking', frame)

    # Press 'q' to exit the loop
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release resources
cap.release()
cv2.destroyAllWindows()
